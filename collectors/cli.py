"""
AI Update Radar - çµ±åˆ CLI
å…¨ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’çµ±åˆã—ã¦å®Ÿè¡Œ
"""

import json
import os
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Optional

import typer
import yaml
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from collectors.github_collector import GitHubCollector
from collectors.models import Category, CollectedEntry, CollectionResult
from collectors.page_diff_collector import PageDiffCollector
from collectors.rss_collector import RSSCollector
from collectors.zenn_collector import ZennCollector
from evaluators.article_evaluator import ArticleEvaluator, EvaluationResult

app = typer.Typer(help="AI Update Radar - AI ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆç›£è¦–ãƒ„ãƒ¼ãƒ«")
console = Console()


def get_paths() -> tuple[Path, Path, Path, Path]:
    """ãƒ‘ã‚¹è¨­å®šã‚’å–å¾—"""
    base_dir = Path(__file__).parent.parent
    sources_dir = base_dir / "sources"
    cache_dir = base_dir / ".private" / "cache"
    keywords_path = sources_dir / "keywords.yaml"
    exports_dir = base_dir / "exports"
    return sources_dir, cache_dir, keywords_path, exports_dir


def format_results_table(results: list[CollectionResult], title: str) -> None:
    """çµæœã‚’ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§è¡¨ç¤º"""
    all_entries = []
    for result in results:
        all_entries.extend(result.entries)

    if not all_entries:
        console.print(f"[dim]{title}: æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªãªã—[/dim]")
        return

    # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
    min_dt = datetime.min.replace(tzinfo=timezone.utc)
    all_entries.sort(key=lambda e: e.published_at or min_dt, reverse=True)

    table = Table(title=f"{title} ({len(all_entries)} ä»¶)")
    table.add_column("æ—¥ä»˜", style="dim", width=6)
    table.add_column("ã‚½ãƒ¼ã‚¹", width=20)
    table.add_column("ã‚¿ã‚¤ãƒˆãƒ«", width=50)
    table.add_column("ã‚«ãƒ†ã‚´ãƒª", width=12)
    table.add_column("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", style="cyan", width=20)

    for entry in all_entries[:20]:  # æœ€å¤§20ä»¶
        date_str = entry.published_at.strftime("%m/%d") if entry.published_at else "-"
        cats = ", ".join(c.value for c in entry.categories[:2])
        kws = ", ".join(entry.keywords[:3])
        table.add_row(date_str, entry.source_name[:20], entry.title[:50], cats, kws)

    console.print(table)


def print_errors(results: list[CollectionResult]) -> None:
    """ã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤º"""
    for result in results:
        for err in result.errors:
            console.print(f"[red]Error ({result.source_name}): {err}[/red]")


@app.command()
def collect(
    days: int = typer.Option(7, help="éå»Næ—¥åˆ†ã‚’åé›†"),
    rss: bool = typer.Option(True, help="RSS ã‚’åé›†"),
    github: bool = typer.Option(True, help="GitHub ãƒªãƒªãƒ¼ã‚¹ã‚’åé›†"),
    pages: bool = typer.Option(True, help="ãƒšãƒ¼ã‚¸å·®åˆ†ã‚’æ¤œå‡º"),
    export: bool = typer.Option(False, help="JSON ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"),
    output: Optional[str] = typer.Option(None, help="ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå…ˆãƒ•ã‚¡ã‚¤ãƒ«å"),
):
    """
    å…¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’åé›†

    ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ RSSã€GitHubã€ãƒšãƒ¼ã‚¸å·®åˆ†ã‚’ã™ã¹ã¦åé›†
    """
    sources_dir, cache_dir, keywords_path, exports_dir = get_paths()

    since = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)
    since = since - timedelta(days=days)

    all_results: list[CollectionResult] = []

    # RSS åé›†
    if rss:
        console.print("[bold]ğŸ“° RSS ãƒ•ã‚£ãƒ¼ãƒ‰åé›†ä¸­...[/bold]")
        rss_collector = RSSCollector(
            sources_dir=sources_dir,
            cache_dir=cache_dir,
            keywords_path=keywords_path,
        )
        rss_results = rss_collector.collect_all(since=since)
        all_results.extend(rss_results)
        format_results_table(rss_results, "RSS ãƒ•ã‚£ãƒ¼ãƒ‰")
        print_errors(rss_results)
        console.print()

    # GitHub åé›†
    if github:
        console.print("[bold]ğŸ™ GitHub ãƒªãƒªãƒ¼ã‚¹åé›†ä¸­...[/bold]")
        github_collector = GitHubCollector(
            sources_dir=sources_dir,
            cache_dir=cache_dir,
            token=os.environ.get("GITHUB_TOKEN"),
            keywords_path=keywords_path,
        )
        github_results = github_collector.collect_all(since=since)
        all_results.extend(github_results)
        format_results_table(github_results, "GitHub ãƒªãƒªãƒ¼ã‚¹")
        print_errors(github_results)
        console.print()

    # ãƒšãƒ¼ã‚¸å·®åˆ†
    if pages:
        console.print("[bold]ğŸ” ãƒšãƒ¼ã‚¸å·®åˆ†æ¤œå‡ºä¸­...[/bold]")
        page_collector = PageDiffCollector(
            sources_dir=sources_dir,
            cache_dir=cache_dir,
            keywords_path=keywords_path,
        )
        page_results = page_collector.collect_all()
        all_results.extend(page_results)
        format_results_table(page_results, "ãƒšãƒ¼ã‚¸å·®åˆ†")
        print_errors(page_results)
        console.print()

    # ã‚µãƒãƒª
    total_entries = sum(len(r.entries) for r in all_results)
    total_errors = sum(len(r.errors) for r in all_results)

    summary = Panel(
        f"ğŸ“Š åé›†å®Œäº†\n"
        f"  â€¢ ã‚¨ãƒ³ãƒˆãƒª: {total_entries} ä»¶\n"
        f"  â€¢ ã‚¨ãƒ©ãƒ¼: {total_errors} ä»¶\n"
        f"  â€¢ æœŸé–“: éå» {days} æ—¥",
        title="ã‚µãƒãƒª",
        border_style="green" if total_errors == 0 else "yellow",
    )
    console.print(summary)

    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    if export:
        exports_dir.mkdir(parents=True, exist_ok=True)
        if output:
            export_path = exports_dir / output
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            export_path = exports_dir / f"collection_{timestamp}.json"

        export_data = {
            "collected_at": datetime.now(timezone.utc).isoformat(),
            "days": days,
            "results": [r.to_dict() for r in all_results],
        }

        with open(export_path, "w") as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)

        console.print(f"[green]âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {export_path}[/green]")


@app.command()
def summary(
    days: int = typer.Option(7, help="éå»Næ—¥åˆ†ã‚’é›†è¨ˆ"),
    category: Optional[str] = typer.Option(None, help="ã‚«ãƒ†ã‚´ãƒªã§ãƒ•ã‚£ãƒ«ã‚¿"),
):
    """
    åé›†çµæœã®ã‚µãƒãƒªã‚’è¡¨ç¤º

    ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ»ã‚½ãƒ¼ã‚¹åˆ¥ã®é›†è¨ˆ
    """
    sources_dir, cache_dir, keywords_path, exports_dir = get_paths()

    since = datetime.now(timezone.utc) - timedelta(days=days)

    # å…¨ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼å®Ÿè¡Œ
    all_entries = []

    # RSS
    rss_collector = RSSCollector(
        sources_dir=sources_dir, cache_dir=cache_dir, keywords_path=keywords_path
    )
    for result in rss_collector.collect_all(since=since):
        all_entries.extend(result.entries)

    # GitHub
    github_collector = GitHubCollector(
        sources_dir=sources_dir,
        cache_dir=cache_dir,
        token=os.environ.get("GITHUB_TOKEN"),
        keywords_path=keywords_path,
    )
    for result in github_collector.collect_all(since=since):
        all_entries.extend(result.entries)

    # ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿
    if category:
        try:
            cat_filter = Category(category)
            all_entries = [e for e in all_entries if cat_filter in e.categories]
        except ValueError:
            console.print(f"[red]ä¸æ­£ãªã‚«ãƒ†ã‚´ãƒª: {category}[/red]")
            console.print(f"æœ‰åŠ¹ãªã‚«ãƒ†ã‚´ãƒª: {[c.value for c in Category]}")
            return

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
    cat_counts = {}
    for entry in all_entries:
        for cat in entry.categories:
            cat_counts[cat.value] = cat_counts.get(cat.value, 0) + 1

    # ã‚½ãƒ¼ã‚¹åˆ¥é›†è¨ˆ
    source_counts = {}
    for entry in all_entries:
        source_counts[entry.source_name] = source_counts.get(entry.source_name, 0) + 1

    # è¡¨ç¤º
    console.print(Panel(f"éå» {days} æ—¥é–“ã®ã‚µãƒãƒª", style="bold"))

    if cat_counts:
        cat_table = Table(title="ã‚«ãƒ†ã‚´ãƒªåˆ¥")
        cat_table.add_column("ã‚«ãƒ†ã‚´ãƒª")
        cat_table.add_column("ä»¶æ•°", justify="right")
        for cat, count in sorted(cat_counts.items(), key=lambda x: -x[1]):
            cat_table.add_row(cat, str(count))
        console.print(cat_table)
    else:
        console.print("[dim]ã‚¨ãƒ³ãƒˆãƒªãªã—[/dim]")

    if source_counts:
        source_table = Table(title="ã‚½ãƒ¼ã‚¹åˆ¥")
        source_table.add_column("ã‚½ãƒ¼ã‚¹")
        source_table.add_column("ä»¶æ•°", justify="right")
        for source, count in sorted(source_counts.items(), key=lambda x: -x[1])[:10]:
            source_table.add_row(source, str(count))
        console.print(source_table)


@app.command()
def sources():
    """ç›£è¦–å¯¾è±¡ã‚½ãƒ¼ã‚¹ã®ä¸€è¦§ã‚’è¡¨ç¤º"""
    sources_dir, _, _, _ = get_paths()

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
    providers_path = sources_dir / "providers.yaml"
    if providers_path.exists():
        with open(providers_path) as f:
            providers = yaml.safe_load(f) or {}

        table = Table(title="ğŸ“° ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼")
        table.add_column("ID")
        table.add_column("åå‰")
        table.add_column("å„ªå…ˆåº¦")
        table.add_column("ã‚½ãƒ¼ã‚¹æ•°")

        for pid, pdata in providers.get("providers", {}).items():
            table.add_row(
                pid,
                pdata.get("name", ""),
                str(pdata.get("priority", "-")),
                str(len(pdata.get("sources", []))),
            )
        console.print(table)
        console.print()

    # ãƒªãƒã‚¸ãƒˆãƒª
    repos_path = sources_dir / "repositories.yaml"
    if repos_path.exists():
        with open(repos_path) as f:
            repos = yaml.safe_load(f) or {}

        table = Table(title="ğŸ™ GitHub ãƒªãƒã‚¸ãƒˆãƒª")
        table.add_column("ID")
        table.add_column("ãƒªãƒã‚¸ãƒˆãƒª")
        table.add_column("å„ªå…ˆåº¦")
        table.add_column("ç›£è¦–å¯¾è±¡")

        for rid, rdata in repos.get("repositories", {}).items():
            table.add_row(
                rid,
                rdata.get("repo", ""),
                str(rdata.get("priority", "-")),
                ", ".join(rdata.get("watch", [])),
            )
        console.print(table)


@app.command()
def init():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆæœŸåŒ–ï¼ˆåˆå›å®Ÿè¡Œæ™‚ã«æ¨å¥¨ï¼‰"""
    _, cache_dir, _, _ = get_paths()

    cache_dir.mkdir(parents=True, exist_ok=True)

    # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤
    count = 0
    for f in cache_dir.glob("*.json"):
        f.unlink()
        count += 1

    if count > 0:
        console.print(f"[yellow]ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {count} ãƒ•ã‚¡ã‚¤ãƒ«[/yellow]")
    else:
        console.print("[dim]ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ[/dim]")

    console.print("[green]âœ… åˆæœŸåŒ–å®Œäº†ã€‚æ¬¡å› collect æ™‚ã«å…¨ã‚¨ãƒ³ãƒˆãƒªãŒæ¤œå‡ºã•ã‚Œã¾ã™ã€‚[/green]")


@app.command()
def evaluate(
    days: int = typer.Option(7, help="éå»Næ—¥åˆ†ã‚’è©•ä¾¡"),
    layer: Optional[int] = typer.Option(None, help="ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ãƒ•ã‚£ãƒ«ã‚¿ (1=ç„¡è¦–, 2=æ¤œçŸ¥, 3=æ·±æ˜ã‚Š)"),
    log: bool = typer.Option(True, help="åˆ¤æ–­ãƒ­ã‚°ã‚’ä¿å­˜"),
    report: bool = typer.Option(False, help="ã‚µãƒãƒªãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º"),
):
    """
    åé›†ãƒ‡ãƒ¼ã‚¿ã‚’è©•ä¾¡ã—ã€Layer åˆ¤å®šã‚’è¡Œã†

    ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°è¦ç´ : é©ç”¨å¯èƒ½æ€§ã€ã‚³ã‚¹ãƒˆå‰Šæ¸›ã€ãƒªã‚¹ã‚¯ã€ç·Šæ€¥æ€§
    """
    from evaluators import EvaluationLogger, Layer, RelevanceScorer

    sources_dir, cache_dir, keywords_path, exports_dir = get_paths()

    since = datetime.now(timezone.utc) - timedelta(days=days)

    # å…¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚¨ãƒ³ãƒˆãƒªã‚’åé›†
    all_entries = []

    console.print("[bold]ğŸ“Š ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...[/bold]")

    # RSS
    rss_collector = RSSCollector(
        sources_dir=sources_dir, cache_dir=cache_dir, keywords_path=keywords_path
    )
    for result in rss_collector.collect_all(since=since):
        all_entries.extend(result.entries)

    # GitHub
    github_collector = GitHubCollector(
        sources_dir=sources_dir,
        cache_dir=cache_dir,
        token=os.environ.get("GITHUB_TOKEN"),
        keywords_path=keywords_path,
    )
    for result in github_collector.collect_all(since=since):
        all_entries.extend(result.entries)

    if not all_entries:
        console.print("[dim]è©•ä¾¡å¯¾è±¡ã®ã‚¨ãƒ³ãƒˆãƒªãŒã‚ã‚Šã¾ã›ã‚“[/dim]")
        return

    console.print(f"[bold]ğŸ” {len(all_entries)} ä»¶ã‚’è©•ä¾¡ä¸­...[/bold]")

    # è©•ä¾¡
    scorer = RelevanceScorer()
    results = scorer.evaluate_batch(all_entries)

    # ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ•ã‚£ãƒ«ã‚¿
    if layer:
        try:
            layer_filter = Layer(layer)
            results = [r for r in results if r.layer == layer_filter]
        except ValueError:
            console.print(f"[red]ä¸æ­£ãªãƒ¬ã‚¤ãƒ¤ãƒ¼: {layer}[/red]")
            console.print("æœ‰åŠ¹ãªãƒ¬ã‚¤ãƒ¤ãƒ¼: 1=ç„¡è¦–, 2=æ¤œçŸ¥, 3=æ·±æ˜ã‚Š")
            return

    # çµæœè¡¨ç¤º
    table = Table(title=f"è©•ä¾¡çµæœ ({len(results)} ä»¶)")
    table.add_column("Layer", width=8)
    table.add_column("Score", width=6)
    table.add_column("Cat", width=10)
    table.add_column("ã‚¿ã‚¤ãƒˆãƒ«", width=40)
    table.add_column("ç†ç”±", width=30)

    # ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ¥ã«ã‚½ãƒ¼ãƒˆï¼ˆé«˜ã„æ–¹ãŒä¸Šï¼‰
    results.sort(key=lambda r: (r.layer.value, r.relevance_score), reverse=True)

    layer_styles = {
        Layer.EXPERIMENT: "bold green",
        Layer.DETECT: "yellow",
        Layer.IGNORE: "dim",
    }

    for result in results[:30]:  # æœ€å¤§30ä»¶
        style = layer_styles.get(result.layer, "")
        table.add_row(
            result.layer.name,
            f"{result.relevance_score:.1f}",
            result.classification.primary_category.value,
            result.entry.title[:40],
            result.reason[:30],
            style=style,
        )

    console.print(table)

    # é›†è¨ˆã‚µãƒãƒª
    by_layer = {Layer.EXPERIMENT: 0, Layer.DETECT: 0, Layer.IGNORE: 0}
    for r in results:
        by_layer[r.layer] += 1

    summary_panel = Panel(
        f"ğŸ¯ æ·±æ˜ã‚Šå¯¾è±¡: [bold green]{by_layer[Layer.EXPERIMENT]}[/bold green] ä»¶\n"
        f"ğŸ“‹ æ¤œçŸ¥ã®ã¿: [yellow]{by_layer[Layer.DETECT]}[/yellow] ä»¶\n"
        f"ğŸ”‡ ç„¡è¦–: [dim]{by_layer[Layer.IGNORE]}[/dim] ä»¶",
        title="è©•ä¾¡ã‚µãƒãƒª",
        border_style="blue",
    )
    console.print(summary_panel)

    # ãƒ­ã‚°ä¿å­˜
    if log:
        logger = EvaluationLogger()
        log_path = logger.log_batch(results)
        console.print(f"[green]âœ… åˆ¤æ–­ãƒ­ã‚°ä¿å­˜: {log_path}[/green]")

    # ã‚µãƒãƒªãƒ¬ãƒãƒ¼ãƒˆ
    if report:
        logger = EvaluationLogger()
        console.print()
        console.print(logger.generate_summary_report(days=days))


@app.command()
def export(
    days: int = typer.Option(7, help="éå»Næ—¥åˆ†ã‚’è©•ä¾¡ãƒ»ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"),
    digest: bool = typer.Option(True, help="é€±æ¬¡ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’å‡ºåŠ›"),
    adopted: bool = typer.Option(True, help="æ¡ç”¨æ±ºå®šãƒªã‚¹ãƒˆã‚’å‡ºåŠ›"),
    alerts: bool = typer.Option(True, help="æŠ€è¡“ã‚¢ãƒ©ãƒ¼ãƒˆã‚’å‡ºåŠ›"),
    notify: bool = typer.Option(False, help="infra-automation ã«é€šçŸ¥"),
    ledger: bool = typer.Option(False, help="decision-ledger ã«è¨˜éŒ²"),
):
    """
    è©•ä¾¡çµæœã‚’ä»–ãƒªãƒã‚¸ãƒˆãƒªå‘ã‘ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

    é€±æ¬¡ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆï¼ˆJSONï¼‰ã€æ¡ç”¨æ±ºå®šãƒªã‚¹ãƒˆï¼ˆYAMLï¼‰ã€æŠ€è¡“ã‚¢ãƒ©ãƒ¼ãƒˆï¼ˆYAMLï¼‰ã‚’å‡ºåŠ›
    """
    from evaluators import Exporter, Layer, RelevanceScorer

    sources_dir, cache_dir, keywords_path, exports_dir = get_paths()

    since = datetime.now(timezone.utc) - timedelta(days=days)

    # å…¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚¨ãƒ³ãƒˆãƒªã‚’åé›†
    all_entries = []

    console.print("[bold]ğŸ“Š ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...[/bold]")

    # RSS
    rss_collector = RSSCollector(
        sources_dir=sources_dir, cache_dir=cache_dir, keywords_path=keywords_path
    )
    for result in rss_collector.collect_all(since=since):
        all_entries.extend(result.entries)

    # GitHub
    github_collector = GitHubCollector(
        sources_dir=sources_dir,
        cache_dir=cache_dir,
        token=os.environ.get("GITHUB_TOKEN"),
        keywords_path=keywords_path,
    )
    for result in github_collector.collect_all(since=since):
        all_entries.extend(result.entries)

    if not all_entries:
        console.print("[dim]ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯¾è±¡ã®ã‚¨ãƒ³ãƒˆãƒªãŒã‚ã‚Šã¾ã›ã‚“[/dim]")
        return

    console.print(f"[bold]ğŸ” {len(all_entries)} ä»¶ã‚’è©•ä¾¡ä¸­...[/bold]")

    # è©•ä¾¡
    scorer = RelevanceScorer()
    results = scorer.evaluate_batch(all_entries)

    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    exporter = Exporter()
    exported_paths = {}

    if digest:
        path = exporter.export_weekly_digest(results)
        exported_paths["digest"] = path
        console.print(f"[green]âœ… é€±æ¬¡ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ: {path}[/green]")

    if adopted:
        path = exporter.export_adopted_list(results)
        exported_paths["adopted"] = path
        console.print(f"[green]âœ… æ¡ç”¨æ±ºå®šãƒªã‚¹ãƒˆ: {path}[/green]")

    if alerts:
        path = exporter.export_alerts(results)
        exported_paths["alerts"] = path
        console.print(f"[green]âœ… æŠ€è¡“ã‚¢ãƒ©ãƒ¼ãƒˆ: {path}[/green]")

    # ã‚µãƒãƒª
    layer_counts = {Layer.EXPERIMENT: 0, Layer.DETECT: 0, Layer.IGNORE: 0}
    for r in results:
        layer_counts[r.layer] += 1

    summary = Panel(
        f"ğŸ“¤ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†\n"
        f"  â€¢ è©•ä¾¡: {len(results)} ä»¶\n"
        f"  â€¢ æ·±æ˜ã‚Šå¯¾è±¡: {layer_counts[Layer.EXPERIMENT]} ä»¶\n"
        f"  â€¢ æ¤œçŸ¥ã®ã¿: {layer_counts[Layer.DETECT]} ä»¶\n"
        f"  â€¢ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {len(exported_paths)} ä»¶",
        title="ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚µãƒãƒª",
        border_style="green",
    )
    console.print(summary)

    # infra-automation ã¸ã®é€šçŸ¥
    if notify:
        console.print("[bold]ğŸ“¢ infra-automation ã«é€šçŸ¥ä¸­...[/bold]")
        try:
            # Layer 3 ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’é€šçŸ¥
            highlights = [r for r in results if r.layer == Layer.EXPERIMENT]
            if highlights:
                # snippet-collector çµŒç”±ã§é€šçŸ¥ï¼ˆMCP é€£æºï¼‰
                console.print(f"[yellow]  é€šçŸ¥å¯¾è±¡: {len(highlights)} ä»¶ã®æ·±æ˜ã‚Šå€™è£œ[/yellow]")
                console.print("[dim]  â€» MCP çµŒç”±ã§ snippet-collector ã«ä¿å­˜æ¨å¥¨[/dim]")
            else:
                console.print("[dim]  é€šçŸ¥å¯¾è±¡ãªã—ï¼ˆæ·±æ˜ã‚Šå€™è£œãªã—ï¼‰[/dim]")
        except Exception as e:
            console.print(f"[red]é€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}[/red]")

    # decision-ledger ã¸ã®è¨˜éŒ²
    if ledger:
        console.print("[bold]ğŸ“ decision-ledger ã«è¨˜éŒ²ä¸­...[/bold]")
        try:
            # Layer 3 ã®åˆ¤æ–­ã‚’è¨˜éŒ²
            experiment_results = [r for r in results if r.layer == Layer.EXPERIMENT]
            if experiment_results:
                cnt = len(experiment_results)
                console.print(f"[yellow]  è¨˜éŒ²å¯¾è±¡: {cnt} ä»¶ã®æ·±æ˜ã‚Šåˆ¤æ–­[/yellow]")
                console.print("[dim]  â€» MCP decision-ledger çµŒç”±ã§è¨˜éŒ²æ¨å¥¨[/dim]")
            else:
                console.print("[dim]  è¨˜éŒ²å¯¾è±¡ãªã—ï¼ˆæ·±æ˜ã‚Šåˆ¤æ–­ãªã—ï¼‰[/dim]")
        except Exception as e:
            console.print(f"[red]è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}[/red]")


@app.command()
def zenn(
    days: int = typer.Option(7, help="éå»Næ—¥åˆ†ã‚’åé›†"),
    export: bool = typer.Option(False, help="JSON ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"),
    min_score: Optional[int] = typer.Option(None, help="æœ€ä½ã‚¹ã‚³ã‚¢ï¼ˆNone ã§è¨­å®šå€¤ã‚’ä½¿ç”¨ã€-999 ã§å…¨ä»¶ï¼‰"),
    output: Optional[str] = typer.Option(None, help="ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå…ˆãƒ•ã‚¡ã‚¤ãƒ«å"),
):
    """
    Zenn è¨˜äº‹ã‚’åé›†ï¼ˆæ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ–¹å¼ â‘ ï¼‰

    ãƒˆãƒ”ãƒƒã‚¯åˆ¥ RSS ã‹ã‚‰è¨˜äº‹ã‚’åé›†ã—ã€soft filter ã§ã‚¹ã‚³ã‚¢ä»˜ã‘
    """
    sources_dir, cache_dir, keywords_path, exports_dir = get_paths()

    since = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)
    since = since - timedelta(days=days)

    console.print("[bold]ğŸ“° Zenn è¨˜äº‹åé›†ä¸­...[/bold]")

    collector = ZennCollector(
        sources_dir=sources_dir,
        cache_dir=cache_dir,
        keywords_path=keywords_path,
    )

    result = collector.collect(since=since, min_score=min_score)

    # çµæœè¡¨ç¤º
    if result.entries:
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        def get_score(entry: CollectedEntry) -> int:
            try:
                data = json.loads(entry.raw_content)
                return data.get("prefilter_score", 0)
            except (json.JSONDecodeError, TypeError):
                return 0

        sorted_entries = sorted(result.entries, key=get_score, reverse=True)

        table = Table(title=f"Zenn è¨˜äº‹ ({len(sorted_entries)} ä»¶)")
        table.add_column("æ—¥ä»˜", style="dim", width=6)
        table.add_column("ã‚¹ã‚³ã‚¢", width=5, justify="right")
        table.add_column("ã‚¿ã‚¤ãƒˆãƒ«", width=50)
        table.add_column("ãƒˆãƒ”ãƒƒã‚¯", width=10)
        table.add_column("ãƒãƒƒãƒ", style="cyan", width=20)

        for entry in sorted_entries[:30]:
            date_str = entry.published_at.strftime("%m/%d") if entry.published_at else "-"
            try:
                filter_data = json.loads(entry.raw_content)
                score = filter_data.get("prefilter_score", 0)
                topic = filter_data.get("source_topic", "")
                matched = ", ".join(filter_data.get("boost_matched", [])[:3])
            except (json.JSONDecodeError, TypeError):
                score = 0
                topic = ""
                matched = ""

            score_style = "green" if score >= 2 else "yellow" if score >= 0 else "red"
            table.add_row(
                date_str,
                f"[{score_style}]{score}[/{score_style}]",
                entry.title[:50],
                topic,
                matched,
            )

        console.print(table)
    else:
        console.print("[dim]æ–°ã—ã„è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“[/dim]")

    # ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
    for err in result.errors:
        console.print(f"[red]Error: {err}[/red]")

    # ã‚µãƒãƒª
    summary_panel = Panel(
        f"ğŸ“Š åé›†å®Œäº†\n"
        f"  â€¢ è¨˜äº‹: {len(result.entries)} ä»¶\n"
        f"  â€¢ ã‚¨ãƒ©ãƒ¼: {len(result.errors)} ä»¶\n"
        f"  â€¢ æœŸé–“: éå» {days} æ—¥",
        title="Zenn åé›†ã‚µãƒãƒª",
        border_style="green" if not result.errors else "yellow",
    )
    console.print(summary_panel)

    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    if export:
        exports_dir.mkdir(parents=True, exist_ok=True)
        if output:
            export_path = exports_dir / output
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            export_path = exports_dir / f"zenn_{timestamp}.json"

        export_data = {
            "collected_at": datetime.now(timezone.utc).isoformat(),
            "days": days,
            "min_score": min_score,
            "result": result.to_dict(),
        }

        with open(export_path, "w") as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)

        console.print(f"[green]âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {export_path}[/green]")


@app.command(name="evaluate-articles")
def evaluate_articles(
    days: Optional[int] = typer.Option(None, help="Zenn è¨˜äº‹ã‚’åé›†ã—ã¦ã‹ã‚‰è©•ä¾¡ï¼ˆéå»Næ—¥åˆ†ï¼‰"),
    input_file: Optional[str] = typer.Option(None, "--input", help="æ—¢å­˜ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ JSON ã‚’å…¥åŠ›"),
    output: Optional[str] = typer.Option(None, help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å"),
    min_score: Optional[int] = typer.Option(None, help="Zenn åé›†æ™‚ã®æœ€ä½ã‚¹ã‚³ã‚¢"),
):
    """
    Zenn è¨˜äº‹ã‚’ AI è©•ä¾¡ï¼ˆæ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ–¹å¼ â‘¡ï¼‰

    send_consultation çµŒç”±ã§ LLM ã«è¨˜äº‹ã‚’è©•ä¾¡ã•ã›ã‚‹ã€‚
    --days: Zenn åé›† + è©•ä¾¡ã®ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
    --input: æ—¢å­˜ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ JSON ã‚’å…¥åŠ›ã¨ã—ã¦è©•ä¾¡
    """
    sources_dir, cache_dir, keywords_path, exports_dir = get_paths()

    if days is None and input_file is None:
        console.print("[red]--days ã¾ãŸã¯ --input ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„[/red]")
        raise typer.Exit(1)

    entries = []

    if input_file:
        # æ—¢å­˜ JSON ã‹ã‚‰èª­ã¿è¾¼ã¿
        import_path = exports_dir / input_file if not Path(input_file).is_absolute() else Path(input_file)
        if not import_path.exists():
            console.print(f"[red]ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {import_path}[/red]")
            raise typer.Exit(1)

        console.print(f"[bold]ğŸ“‚ {import_path.name} ã‹ã‚‰èª­ã¿è¾¼ã¿ä¸­...[/bold]")
        with open(import_path) as f:
            data = json.load(f)

        # zenn ã‚³ãƒãƒ³ãƒ‰ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå½¢å¼ã‹ã‚‰ CollectedEntry ã‚’å¾©å…ƒ
        result_data = data.get("result", {})
        for entry_data in result_data.get("entries", []):
            entries.append(CollectedEntry.from_dict(entry_data))

    else:
        # Zenn è¨˜äº‹ã‚’åé›†
        since = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)
        since = since - timedelta(days=days)

        console.print(f"[bold]ğŸ“° Zenn è¨˜äº‹åé›†ä¸­ï¼ˆéå» {days} æ—¥ï¼‰...[/bold]")
        collector = ZennCollector(
            sources_dir=sources_dir,
            cache_dir=cache_dir,
            keywords_path=keywords_path,
        )
        result = collector.collect(since=since, min_score=min_score)
        entries = result.entries

        if result.errors:
            for err in result.errors:
                console.print(f"[yellow]Warning: {err}[/yellow]")

    if not entries:
        console.print("[dim]è©•ä¾¡å¯¾è±¡ã®è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“[/dim]")
        raise typer.Exit(0)

    console.print(f"[bold]ğŸ” {len(entries)} ä»¶ã‚’ AI è©•ä¾¡ä¸­...[/bold]")

    # send_consultation é–¢æ•°ã‚’æ³¨å…¥
    # MCP çµŒç”±ã§å‘¼ã³å‡ºã™å ´åˆã¯å¤–éƒ¨ã‹ã‚‰ send_fn ã‚’æ³¨å…¥ã™ã‚‹æƒ³å®š
    # CLI å˜ä½“ã§ã¯ send_fn ãŒ None â†’ ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    send_fn = _get_send_fn()
    if send_fn is None:
        console.print("[red]send_consultation ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚[/red]")
        console.print("[dim]SEND_CONSULTATION_URL ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼ˆMCP gateway çµŒç”±ã§ LLM è©•ä¾¡ã‚’å®Ÿè¡Œï¼‰[/dim]")
        raise typer.Exit(1)

    evaluator = ArticleEvaluator(send_fn=send_fn)
    eval_result = evaluator.evaluate_batch(entries)

    # çµæœè¡¨ç¤º
    if eval_result.evaluations:
        # relevance é™é †ã§ã‚½ãƒ¼ãƒˆ
        sorted_evals = sorted(
            eval_result.evaluations,
            key=lambda e: (e.relevance, e.actionability),
            reverse=True,
        )

        table = Table(title=f"AI è©•ä¾¡çµæœ ({len(sorted_evals)} ä»¶)")
        table.add_column("é–¢é€£æ€§", width=5, justify="center")
        table.add_column("å®Ÿç”¨æ€§", width=5, justify="center")
        table.add_column("åˆ¤å®š", width=6)
        table.add_column("ã‚¿ã‚¤ãƒˆãƒ«", width=40)
        table.add_column("è¦ç´„", width=25)
        table.add_column("å…ƒ", width=4)

        action_styles = {
            "adopt": "bold green",
            "watch": "yellow",
            "skip": "dim",
        }

        for ev in sorted_evals[:30]:
            rel_style = "green" if ev.relevance >= 4 else "yellow" if ev.relevance >= 3 else "dim"
            act_style = action_styles.get(ev.recommended_action, "")
            src_mark = "LLM" if ev.evaluation_source == "llm" else "FB"
            table.add_row(
                f"[{rel_style}]{ev.relevance}[/{rel_style}]",
                f"{ev.actionability}",
                f"[{act_style}]{ev.recommended_action}[/{act_style}]",
                ev.title[:40],
                ev.summary_ja[:25],
                src_mark,
            )

        console.print(table)

    # ã‚µãƒãƒª
    adopt_count = sum(1 for e in eval_result.evaluations if e.recommended_action == "adopt")
    watch_count = sum(1 for e in eval_result.evaluations if e.recommended_action == "watch")
    skip_count = sum(1 for e in eval_result.evaluations if e.recommended_action == "skip")

    summary_panel = Panel(
        f"ğŸ“Š AI è©•ä¾¡å®Œäº†\n"
        f"  â€¢ è©•ä¾¡: {eval_result.total} ä»¶ï¼ˆLLM: {eval_result.llm_evaluated}, ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {eval_result.fallback_used}ï¼‰\n"
        f"  â€¢ æ¡ç”¨æ¨å¥¨: [bold green]{adopt_count}[/bold green] ä»¶\n"
        f"  â€¢ æ³¨è¦–: [yellow]{watch_count}[/yellow] ä»¶\n"
        f"  â€¢ ã‚¹ã‚­ãƒƒãƒ—: [dim]{skip_count}[/dim] ä»¶",
        title="è©•ä¾¡ã‚µãƒãƒª",
        border_style="green",
    )
    console.print(summary_panel)

    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    exports_dir.mkdir(parents=True, exist_ok=True)
    if output:
        export_path = exports_dir / output
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        export_path = exports_dir / f"article_evaluations_{timestamp}.json"

    with open(export_path, "w") as f:
        json.dump(eval_result.to_dict(), f, indent=2, ensure_ascii=False)

    console.print(f"[green]âœ… è©•ä¾¡çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ: {export_path}[/green]")


def _get_send_fn():
    """send_consultation é–¢æ•°ã‚’å–å¾—ï¼ˆMCP çµŒç”±ï¼‰

    ç’°å¢ƒå¤‰æ•° SEND_CONSULTATION_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã‚Œã° HTTP çµŒç”±ã§å‘¼ã³å‡ºã™ã€‚
    æœªè¨­å®šã®å ´åˆã¯ None ã‚’è¿”ã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©•ä¾¡ã®ã¿å¯èƒ½ï¼‰ã€‚
    """
    url = os.environ.get("SEND_CONSULTATION_URL")
    if not url:
        return None

    import urllib.request

    def send_fn(situation: str, options: list, question: str, consultation_type: str) -> str:
        payload = json.dumps({
            "situation": situation,
            "options": options,
            "question": question,
            "consultation_type": consultation_type,
        }).encode("utf-8")

        req = urllib.request.Request(
            url,
            data=payload,
            headers={"Content-Type": "application/json"},
            method="POST",
        )
        with urllib.request.urlopen(req, timeout=120) as resp:
            result = json.loads(resp.read().decode("utf-8"))
            return result.get("response", result.get("result", ""))

    return send_fn


def _post_to_mastodon(articles: list[dict]) -> list[dict]:
    """Mastodon ã«è¨˜äº‹ã‚’æŠ•ç¨¿"""
    api_url = os.environ.get("MASTODON_API_URL")
    token = os.environ.get("MASTODON_ACCESS_TOKEN")
    if not api_url or not token:
        return []

    import urllib.request

    results = []
    for article in articles:
        status = (
            f"ğŸ“° {article['title']}\n\n"
            f"{article.get('summary_ja', '')}\n\n"
            f"é–¢é€£æ€§: {'â­' * article.get('relevance', 0)}\n"
            f"{article['url']}\n\n"
            f"#AI #è‡ªå‹•åŒ– #æŠ€è¡“è¨˜äº‹"
        )

        payload = json.dumps({"status": status, "visibility": "unlisted"}).encode("utf-8")
        req = urllib.request.Request(
            f"{api_url}/api/v1/statuses",
            data=payload,
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {token}",
            },
            method="POST",
        )
        try:
            with urllib.request.urlopen(req, timeout=30) as resp:
                result = json.loads(resp.read().decode("utf-8"))
                results.append({"url": article["url"], "toot_id": result.get("id"), "success": True})
        except Exception as e:
            results.append({"url": article["url"], "success": False, "error": str(e)})

    return results


@app.command(name="notify-articles")
def notify_articles(
    input_file: Optional[str] = typer.Option(None, "--input", help="è¨˜äº‹å€™è£œ JSONï¼ˆarticle_candidates.jsonï¼‰"),
    decisions_file: Optional[str] = typer.Option(None, "--decisions", help="æ‰¿èªçµæœ JSONï¼ˆarticle_decisions.jsonï¼‰"),
    dry_run: bool = typer.Option(False, help="æŠ•ç¨¿ã›ãšè¡¨ç¤ºã®ã¿"),
):
    """
    æ‰¿èªæ¸ˆã¿è¨˜äº‹ã‚’é€šçŸ¥ï¼ˆæ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ–¹å¼ â‘¢ï¼‰

    --input: AI è©•ä¾¡çµæœ JSON
    --decisions: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸæ‰¿èªçµæœ JSON
    Mastodon ã«æŠ•ç¨¿ã™ã‚‹å ´åˆã¯ MASTODON_API_URL ã¨ MASTODON_ACCESS_TOKEN ã‚’è¨­å®š
    """
    base_dir = Path(__file__).parent.parent
    default_candidates_path = base_dir / "frontend" / "public" / "data" / "article_candidates.json"

    # 1. article_candidates.json ã‚’èª­ã¿è¾¼ã‚€
    if input_file:
        candidates_path = Path(input_file) if Path(input_file).is_absolute() else base_dir / input_file
    else:
        candidates_path = default_candidates_path

    if not candidates_path.exists():
        console.print(f"[red]ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {candidates_path}[/red]")
        raise typer.Exit(1)

    console.print(f"[bold]ğŸ“‚ è¨˜äº‹å€™è£œã‚’èª­ã¿è¾¼ã¿ä¸­: {candidates_path.name}[/bold]")
    try:
        with open(candidates_path) as f:
            candidates_data = json.load(f)
    except (json.JSONDecodeError, OSError) as e:
        console.print(f"[red]èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}[/red]")
        raise typer.Exit(1)

    # candidates_data ã¯ list ã¾ãŸã¯ dictï¼ˆevaluations ã‚­ãƒ¼ä»˜ãï¼‰
    if isinstance(candidates_data, list):
        all_candidates = candidates_data
    elif isinstance(candidates_data, dict):
        all_candidates = candidates_data.get("evaluations", candidates_data.get("articles", []))
    else:
        all_candidates = []

    # 2. decisions ã‚’èª­ã¿è¾¼ã¿ã€æ‰¿èªæ¸ˆã¿è¨˜äº‹ã‚’æ±ºå®š
    approved_articles = []

    if decisions_file:
        decisions_path = Path(decisions_file) if Path(decisions_file).is_absolute() else base_dir / decisions_file
        if not decisions_path.exists():
            console.print(f"[red]æ‰¿èªçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {decisions_path}[/red]")
            raise typer.Exit(1)

        console.print(f"[bold]ğŸ“‹ æ‰¿èªçµæœã‚’èª­ã¿è¾¼ã¿ä¸­: {decisions_path.name}[/bold]")
        try:
            with open(decisions_path) as f:
                decisions_data = json.load(f)
        except (json.JSONDecodeError, OSError) as e:
            console.print(f"[red]æ‰¿èªçµæœã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}[/red]")
            raise typer.Exit(1)

        approved_articles = decisions_data.get("approved", [])
        console.print(f"[dim]  æ‰¿èªæ—¥æ™‚: {decisions_data.get('exported_at', 'ä¸æ˜')}[/dim]")
    else:
        # decisions ãŒãªã„å ´åˆã¯ recommended_action == "adopt" ã®è¨˜äº‹ã‚’è‡ªå‹•é¸æŠ
        console.print("[dim]æ‰¿èªçµæœãªã— â†’ recommended_action == 'adopt' ã®è¨˜äº‹ã‚’è‡ªå‹•é¸æŠ[/dim]")
        for article in all_candidates:
            if article.get("recommended_action") == "adopt":
                approved_articles.append(article)

    if not approved_articles:
        console.print("[yellow]æ‰¿èªæ¸ˆã¿è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“[/yellow]")
        raise typer.Exit(0)

    # 3. æ‰¿èªæ¸ˆã¿è¨˜äº‹ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
    table = Table(title=f"æ‰¿èªæ¸ˆã¿è¨˜äº‹ ({len(approved_articles)} ä»¶)")
    table.add_column("é–¢é€£æ€§", width=5, justify="center")
    table.add_column("ã‚¿ã‚¤ãƒˆãƒ«", width=45)
    table.add_column("åˆ¤å®š", width=6)
    table.add_column("æŠ•ç¨¿", width=6)

    # 4. Mastodon æŠ•ç¨¿
    post_results = []
    if not dry_run:
        api_url = os.environ.get("MASTODON_API_URL")
        token = os.environ.get("MASTODON_ACCESS_TOKEN")
        if api_url and token:
            console.print("[bold]ğŸ“¤ Mastodon ã«æŠ•ç¨¿ä¸­...[/bold]")
            post_results = _post_to_mastodon(approved_articles)
        else:
            console.print("[dim]Mastodon ç’°å¢ƒå¤‰æ•°æœªè¨­å®šï¼ˆMASTODON_API_URL, MASTODON_ACCESS_TOKENï¼‰â†’ æŠ•ç¨¿ã‚¹ã‚­ãƒƒãƒ—[/dim]")
    else:
        console.print("[yellow]dry-run ãƒ¢ãƒ¼ãƒ‰: æŠ•ç¨¿ã‚’ã‚¹ã‚­ãƒƒãƒ—[/yellow]")

    # æŠ•ç¨¿çµæœã‚’ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”¨ã«å¤‰æ›
    post_result_map = {r["url"]: r for r in post_results}

    # ãƒ†ãƒ¼ãƒ–ãƒ«ã«è¡Œã‚’è¿½åŠ 
    success_count = 0
    error_count = 0
    for article in approved_articles:
        relevance = article.get("relevance", 0)
        rel_style = "green" if relevance >= 4 else "yellow" if relevance >= 3 else "dim"
        action = article.get("recommended_action", "-")

        # æŠ•ç¨¿çµæœ
        pr = post_result_map.get(article.get("url", ""))
        if pr:
            if pr.get("success"):
                post_status = "[green]OK[/green]"
                success_count += 1
            else:
                post_status = "[red]NG[/red]"
                error_count += 1
        elif dry_run:
            post_status = "[dim]skip[/dim]"
        else:
            post_status = "[dim]-[/dim]"

        table.add_row(
            f"[{rel_style}]{relevance}[/{rel_style}]",
            article.get("title", "")[:45],
            action,
            post_status,
        )

    console.print(table)

    # 5. ã‚µãƒãƒªè¡¨ç¤º
    summary_parts = [
        f"ğŸ“Š é€šçŸ¥å®Œäº†",
        f"  â€¢ æ‰¿èªè¨˜äº‹: {len(approved_articles)} ä»¶",
    ]
    if post_results:
        summary_parts.append(f"  â€¢ æŠ•ç¨¿æˆåŠŸ: [green]{success_count}[/green] ä»¶")
        if error_count > 0:
            summary_parts.append(f"  â€¢ æŠ•ç¨¿ã‚¨ãƒ©ãƒ¼: [red]{error_count}[/red] ä»¶")
    elif dry_run:
        summary_parts.append(f"  â€¢ æŠ•ç¨¿: dry-runï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
    else:
        summary_parts.append(f"  â€¢ æŠ•ç¨¿: ç’°å¢ƒå¤‰æ•°æœªè¨­å®šï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")

    border_style = "green" if error_count == 0 else "yellow"
    summary_panel = Panel(
        "\n".join(summary_parts),
        title="é€šçŸ¥ã‚µãƒãƒª",
        border_style=border_style,
    )
    console.print(summary_panel)


@app.command()
def marketing(
    trends: bool = typer.Option(True, help="ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œçŸ¥ã‚’å®Ÿè¡Œ"),
    content: bool = typer.Option(True, help="SNSæŠ•ç¨¿å€™è£œã‚’ç”Ÿæˆ"),
    analytics: bool = typer.Option(False, help="åŠ¹æœæ¸¬å®šã‚µãƒãƒªã‚’è¡¨ç¤º"),
):
    """
    ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æ©Ÿèƒ½

    ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œçŸ¥ã€SNSæŠ•ç¨¿å€™è£œç”Ÿæˆã€åŠ¹æœæ¸¬å®šé€£æº
    """
    from pathlib import Path

    from evaluators.trend_detector import TrendDetector
    from marketing.analytics import AnalyticsTracker
    from marketing.content_generator import ContentGenerator

    base_dir = Path(__file__).parent.parent
    marketing_dir = base_dir / ".private" / "marketing"

    console.print(Panel("ğŸ¯ ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æ©Ÿèƒ½", style="bold"))

    # ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œçŸ¥
    if trends:
        console.print("[bold]ğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œçŸ¥ä¸­...[/bold]")
        detector = TrendDetector(
            data_dir=marketing_dir,
            output_dir=marketing_dir / "trends",
        )
        trend_results = detector.detect_trends()

        rising = trend_results.get("trends", {}).get("rising", [])
        if rising:
            table = Table(title="ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰")
            table.add_column("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
            table.add_column("å¤‰åŒ–")
            table.add_column("å‰é€±â†’ä»Šé€±")

            for t in rising[:5]:
                ratio = t.get("ratio", 0)
                ratio_str = "âˆ" if ratio == float("inf") else f"{ratio}x"
                table.add_row(
                    t.get("keyword", ""),
                    t.get("change", ""),
                    f"{t.get('prev_count', 0)} â†’ {t.get('current_count', 0)} ({ratio_str})",
                )
            console.print(table)
        else:
            console.print("[dim]  ãƒˆãƒ¬ãƒ³ãƒ‰å¤‰åŒ–ãªã—[/dim]")

        # ä¿å­˜
        path = detector.save_trends(trend_results)
        console.print(f"[green]âœ… ãƒˆãƒ¬ãƒ³ãƒ‰ä¿å­˜: {path}[/green]")

    # SNSæŠ•ç¨¿å€™è£œç”Ÿæˆ
    if content:
        console.print()
        console.print("[bold]ğŸ“ SNSæŠ•ç¨¿å€™è£œç”Ÿæˆä¸­...[/bold]")

        generator = ContentGenerator(output_dir=marketing_dir / "content")

        # ãƒˆãƒ¬ãƒ³ãƒ‰ã‹ã‚‰ç”Ÿæˆ
        if trends:
            candidates = generator.generate_from_trends(trend_results)
        else:
            candidates = []

        # é€±æ¬¡ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‹ã‚‰ã‚‚ç”Ÿæˆ
        exports_dir = base_dir / "exports"
        import json

        digests = sorted(exports_dir.glob("digest-*.json"), reverse=True)
        if digests:
            week = digests[0].stem.replace("digest-", "")
            with open(digests[0], encoding="utf-8") as f:
                digest_data = json.load(f)
            candidates.extend(generator.generate_from_digest(week, digest_data))

            # ä¿å­˜
            if candidates:
                path = generator.save_candidates(candidates, week)
                console.print(f"[green]âœ… æŠ•ç¨¿å€™è£œä¿å­˜: {path}[/green]")

                table = Table(title=f"æŠ•ç¨¿å€™è£œ ({len(candidates)}ä»¶)")
                table.add_column("ã‚¿ã‚¤ãƒ—")
                table.add_column("å„ªå…ˆåº¦")
                table.add_column("å†…å®¹ï¼ˆå…ˆé ­50æ–‡å­—ï¼‰")

                for c in candidates[:5]:
                    table.add_row(
                        c.get("type", ""),
                        c.get("priority", ""),
                        c.get("content", "")[:50] + "...",
                    )
                console.print(table)
        else:
            console.print("[dim]  é€±æ¬¡ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“[/dim]")

    # åŠ¹æœæ¸¬å®šã‚µãƒãƒª
    if analytics:
        console.print()
        console.print("[bold]ğŸ“Š åŠ¹æœæ¸¬å®šã‚µãƒãƒª[/bold]")

        tracker = AnalyticsTracker(data_dir=marketing_dir / "analytics")
        summary = tracker.get_performance_summary()

        if summary.get("posts_count", 0) > 0:
            panel = Panel(
                f"ğŸ“ˆ éå» {summary.get('period_weeks', 4)} é€±é–“\n"
                f"  â€¢ æŠ•ç¨¿æ•°: {summary.get('posts_count', 0)}\n"
                f"  â€¢ ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³: {summary.get('total_impressions', 0)}\n"
                f"  â€¢ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡: {summary.get('engagement_rate', 0)}%",
                title="ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                border_style="blue",
            )
            console.print(panel)
        else:
            console.print("[dim]  åŠ¹æœæ¸¬å®šãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“[/dim]")
            console.print("[dim]  â€» æŠ•ç¨¿å¾Œã« analytics.record_post() ã§è¨˜éŒ²ã—ã¦ãã ã•ã„[/dim]")


if __name__ == "__main__":
    app()
